{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc11f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sssd.core.model_specs import setup_model\n",
    "import torch\n",
    "import yaml\n",
    "from sssd.core.model_specs import MASK_FN\n",
    "import torch.nn as nn\n",
    "from sssd.data.utils import get_dataloader\n",
    "from sssd.utils.utils import calc_diffusion_hyperparams\n",
    "from sssd.utils.utils import sampling\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, Union\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sssd.core.layers.s4.s4_layer import S4Layer\n",
    "from sssd.core.utils import calc_diffusion_step_embedding\n",
    "from sssd.data.generator import ArDataGenerator\n",
    "from sssd.data.dataloader import ArDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_coefs = [0.8]\n",
    "series_length = 128\n",
    "season_period = 12\n",
    "\n",
    "# Generate data with intercept (mean = 3)\n",
    "data_with_intercept = ArDataGenerator(ar_coefs, series_length, std=5, intercept=100, season_period=season_period).generate()\n",
    "\n",
    "# Generate data without intercept (mean = 0)\n",
    "data_without_intercept = ArDataGenerator(ar_coefs, series_length, std=5, season_period=season_period).generate()\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(data_with_intercept, label=\"With Intercept (Mean = 3, Std = 5)\")\n",
    "plt.plot(data_without_intercept, label=\"Without Intercept (Mean = 0, Std = 5)\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"AR Process with and Without Intercept\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13df006",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_series = 1024\n",
    "coefficients = [0.8] \n",
    "series_length = 128\n",
    "std = 1\n",
    "intercept = 100\n",
    "season = 12\n",
    "batch_size = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_workers = 4\n",
    "training_rate = 0.8\n",
    "seeds = list(range(num_series))\n",
    "\n",
    "data_loader = ArDataLoader(\n",
    "    coefficients,\n",
    "    num_series,\n",
    "    series_length,\n",
    "    std,\n",
    "    intercept,\n",
    "    season,\n",
    "    batch_size,\n",
    "    device,\n",
    "    num_workers,\n",
    "    training_rate,\n",
    "    seeds,\n",
    ")\n",
    "\n",
    "train_loader = data_loader.train_dataloader\n",
    "test_loader = data_loader.test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92055c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../configs/model.yaml\", \"rt\") as f:\n",
    "    model_config = yaml.safe_load(f.read())\n",
    "with open(\"../configs/training.yaml\", \"rt\") as f:\n",
    "    training_config = yaml.safe_load(f.read())\n",
    "\n",
    "with open(\"../configs/inference.yaml\", \"rt\") as f:\n",
    "    inference_config = yaml.safe_load(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbfb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mask(batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Update mask based on the given batch.\"\"\"\n",
    "    transposed_mask = MASK_FN[\"forecast\"](batch[0], 24)\n",
    "    return (\n",
    "        transposed_mask.permute(1, 0)\n",
    "        .repeat(batch.size()[0], 1, 1)\n",
    "        .to(device, dtype=torch.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size=3, dilation=1):\n",
    "        super().__init__()\n",
    "        self.padding = dilation * (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv1d(\n",
    "            input_channels,\n",
    "            output_channels,\n",
    "            kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        self.conv = nn.utils.parametrizations.weight_norm(self.conv)\n",
    "        nn.init.kaiming_normal_(self.conv.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ZeroConv1d(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(ZeroConv1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channel, out_channel, kernel_size=1, padding=0)\n",
    "        self.conv.weight.data.zero_()\n",
    "        self.conv.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_channels,\n",
    "        skip_channels,\n",
    "        diffusion_step_embed_dim_output,\n",
    "        input_channels,\n",
    "        s4_max_sequence_length,\n",
    "        s4_state_dim,\n",
    "        s4_dropout,\n",
    "        s4_bidirectional,\n",
    "        s4_use_layer_norm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.residual_channels = residual_channels\n",
    "\n",
    "        self.fc_t = nn.Linear(diffusion_step_embed_dim_output, self.residual_channels)\n",
    "\n",
    "        self.S41 = S4Layer(\n",
    "            features=2 * self.residual_channels,\n",
    "            lmax=s4_max_sequence_length,\n",
    "            N=s4_state_dim,\n",
    "            dropout=s4_dropout,\n",
    "            bidirectional=s4_bidirectional,\n",
    "            layer_norm=s4_use_layer_norm,\n",
    "        )\n",
    "\n",
    "        self.conv_layer = Conv(\n",
    "            self.residual_channels, 2 * self.residual_channels, kernel_size=3\n",
    "        )\n",
    "\n",
    "        self.S42 = S4Layer(\n",
    "            features=2 * self.residual_channels,\n",
    "            lmax=s4_max_sequence_length,\n",
    "            N=s4_state_dim,\n",
    "            dropout=s4_dropout,\n",
    "            bidirectional=s4_bidirectional,\n",
    "            layer_norm=s4_use_layer_norm,\n",
    "        )\n",
    "\n",
    "        self.cond_conv = Conv(\n",
    "            2 * input_channels, 2 * self.residual_channels, kernel_size=1\n",
    "        )\n",
    "        self.res_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=1)\n",
    "        self.res_conv = nn.utils.parametrizations.weight_norm(self.res_conv)\n",
    "        nn.init.kaiming_normal_(self.res_conv.weight)\n",
    "\n",
    "        self.skip_conv = nn.Conv1d(residual_channels, skip_channels, kernel_size=1)\n",
    "        self.skip_conv = nn.utils.parametrizations.weight_norm(self.skip_conv)\n",
    "        nn.init.kaiming_normal_(self.skip_conv.weight)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x, cond, diffusion_step_embed = input_data\n",
    "        h = x\n",
    "        B, C, L = x.shape\n",
    "        assert C == self.residual_channels\n",
    "\n",
    "        part_t = self.fc_t(diffusion_step_embed)\n",
    "        part_t = part_t.view([B, self.residual_channels, 1])\n",
    "        h = h + part_t\n",
    "\n",
    "        h = self.conv_layer(h)\n",
    "        h = self.S41(h.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "\n",
    "        assert cond is not None\n",
    "        cond = self.cond_conv(cond)\n",
    "        h += cond\n",
    "\n",
    "        h = self.S42(h.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "\n",
    "        out = torch.tanh(h[:, : self.residual_channels, :]) * torch.sigmoid(\n",
    "            h[:, self.residual_channels :, :]\n",
    "        )\n",
    "\n",
    "        res = self.res_conv(out)\n",
    "        assert x.shape == res.shape\n",
    "        skip = self.skip_conv(out)\n",
    "\n",
    "        return (x + res) * math.sqrt(0.5), skip  # normalize for training stability\n",
    "\n",
    "\n",
    "class ResidualGroup(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_channels,\n",
    "        skip_channels,\n",
    "        residual_layers,\n",
    "        diffusion_step_embed_dim_input,\n",
    "        diffusion_step_embed_dim_hidden,\n",
    "        diffusion_step_embed_dim_output,\n",
    "        input_channels,\n",
    "        s4_max_sequence_length,\n",
    "        s4_state_dim,\n",
    "        s4_dropout,\n",
    "       s4_bidirectional,\n",
    "        s4_use_layer_norm,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super(ResidualGroup, self).__init__()\n",
    "        self.residual_layers = residual_layers\n",
    "        self.diffusion_step_embed_dim_input = diffusion_step_embed_dim_input\n",
    "\n",
    "        self.fc_t1 = nn.Linear(\n",
    "            diffusion_step_embed_dim_input, diffusion_step_embed_dim_hidden\n",
    "        )\n",
    "        self.fc_t2 = nn.Linear(\n",
    "            diffusion_step_embed_dim_hidden, diffusion_step_embed_dim_output\n",
    "        )\n",
    "\n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        for n in range(self.residual_layers):\n",
    "            self.residual_blocks.append(\n",
    "                ResidualBlock(\n",
    "                    residual_channels,\n",
    "                    skip_channels,\n",
    "                    diffusion_step_embed_dim_output=diffusion_step_embed_dim_output,\n",
    "                    input_channels=input_channels,\n",
    "                    s4_max_sequence_length=s4_max_sequence_length,\n",
    "                    s4_state_dim=s4_state_dim,\n",
    "                    s4_dropout=s4_dropout,\n",
    "                    s4_bidirectional=s4_bidirectional,\n",
    "                    s4_use_layer_norm=s4_use_layer_norm,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        noise, conditional, diffusion_steps = input_data\n",
    "\n",
    "        diffusion_step_embed = calc_diffusion_step_embedding(\n",
    "            diffusion_steps, self.diffusion_step_embed_dim_input, device=self.device\n",
    "        )\n",
    "        diffusion_step_embed = swish(self.fc_t1(diffusion_step_embed))\n",
    "        diffusion_step_embed = swish(self.fc_t2(diffusion_step_embed))\n",
    "\n",
    "        h = noise\n",
    "        skip = 0\n",
    "        for n in range(self.residual_layers):\n",
    "            h, skip_n = self.residual_blocks[n]((h, conditional, diffusion_step_embed))\n",
    "            skip += skip_n\n",
    "\n",
    "        return skip * math.sqrt(1.0 / self.residual_layers)\n",
    "\n",
    "\n",
    "class SSSDS4Imputer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        residual_channels,\n",
    "        skip_channels,\n",
    "        output_channels,\n",
    "        residual_layers,\n",
    "        diffusion_step_embed_dim_input,\n",
    "        diffusion_step_embed_dim_hidden,\n",
    "        diffusion_step_embed_dim_output,\n",
    "        s4_max_sequence_length,\n",
    "        s4_state_dim,\n",
    "        s4_dropout,\n",
    "        s4_bidirectional,\n",
    "        s4_use_layer_norm,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_conv = nn.Sequential(\n",
    "            Conv(input_channels, residual_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.residual_layer = ResidualGroup(\n",
    "            residual_channels=residual_channels,\n",
    "            skip_channels=skip_channels,\n",
    "            residual_layers=residual_layers,\n",
    "            diffusion_step_embed_dim_input=diffusion_step_embed_dim_input,\n",
    "            diffusion_step_embed_dim_hidden=diffusion_step_embed_dim_hidden,\n",
    "            diffusion_step_embed_dim_output=diffusion_step_embed_dim_output,\n",
    "            input_channels=input_channels,\n",
    "            s4_max_sequence_length=s4_max_sequence_length,\n",
    "            s4_state_dim=s4_state_dim,\n",
    "            s4_dropout=s4_dropout,\n",
    "            s4_bidirectional=s4_bidirectional,\n",
    "            s4_use_layer_norm=s4_use_layer_norm,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            Conv(skip_channels, skip_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            ZeroConv1d(skip_channels, output_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        noise, conditional, mask, diffusion_steps = input_data\n",
    "\n",
    "        conditional = conditional * mask\n",
    "        conditional = torch.cat([conditional, mask.float()], dim=1)\n",
    "\n",
    "        x = noise\n",
    "        x = self.init_conv(x)\n",
    "        x = self.residual_layer((x, conditional, diffusion_steps))\n",
    "        y = self.final_conv(x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_config[\"residual_layers\"] = 2\n",
    "# model_config[\"residual_channels\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a841ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = SSSDS4Imputer(**model_config.get(\"wavenet\"), device=device)\n",
    "net = net.to(device)\n",
    "net = nn.DataParallel(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ebb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_hyperparams = calc_diffusion_hyperparams(\n",
    "    **model_config[\"diffusion\"], device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from sssd.utils.utils import std_normal\n",
    "\n",
    "\n",
    "def training_loss(\n",
    "    model: torch.nn.Module,\n",
    "    training_data: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    diffusion_parameters: Dict[str, torch.Tensor],\n",
    "    generate_only_missing: int = 1,\n",
    "    device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the training loss of epsilon and epsilon_theta.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        training_data (tuple): Training data tuple containing (time_series, condition, mask, loss_mask).\n",
    "        diffusion_parameters (dict): Dictionary of diffusion hyperparameters returned by calc_diffusion_hyperparams.\n",
    "                                     Note, the tensors need to be cuda tensors.\n",
    "        generate_only_missing (int): Flag to indicate whether to only generate missing values (default=1).\n",
    "        device (str): Device to run the computations on (default=\"cuda\").\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Training loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack diffusion hyperparameters\n",
    "    T, alpha_bar = diffusion_parameters[\"T\"], diffusion_parameters[\"Alpha_bar\"]\n",
    "\n",
    "    # Unpack training data\n",
    "    time_series, condition, mask, loss_mask = training_data\n",
    "\n",
    "    batch_size = time_series.shape[0]\n",
    "\n",
    "    # Sample random diffusion steps for each batch element\n",
    "    diffusion_steps = torch.randint(T, size=(batch_size, 1, 1)).to(device)\n",
    "    # Generate Gaussian noise, applying mask if specified\n",
    "    noise = (\n",
    "        time_series * mask.float()\n",
    "        + std_normal(time_series.shape, device) * (1 - mask).float()\n",
    "        if generate_only_missing\n",
    "        else std_normal(time_series.shape, device)\n",
    "    )\n",
    "\n",
    "    # Compute x_t from q(x_t|x_0)\n",
    "    transformed_series = (\n",
    "        torch.sqrt(alpha_bar[diffusion_steps]) * time_series\n",
    "        + torch.sqrt(1 - alpha_bar[diffusion_steps]) * noise\n",
    "    )\n",
    "\n",
    "    # Predict epsilon according to epsilon_theta\n",
    "    epsilon_theta = model(\n",
    "        (transformed_series, condition, mask, diffusion_steps.view(batch_size, 1))\n",
    "    )\n",
    "\n",
    "    # Compute loss\n",
    "    if generate_only_missing:\n",
    "        return nn.MSELoss()(epsilon_theta[loss_mask], noise[loss_mask])#, epsilon_theta[loss_mask], noise[loss_mask]\n",
    "    else:\n",
    "        return nn.MSELoss()(epsilon_theta, noise)#, epsilon_theta[loss_mask], noise[loss_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ec660",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epochs = 100\n",
    "for epoch in range(epochs):  # Train for 100 epochs (0-indexed)\n",
    "    epoch_loss = 0\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}\") as pbar:\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(device)\n",
    "            mask = update_mask(batch)\n",
    "            loss_mask = ~mask.bool()\n",
    "\n",
    "            batch = batch.permute(0, 2, 1)\n",
    "            assert batch.size() == mask.size() == loss_mask.size()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = training_loss(\n",
    "                model=net,\n",
    "                training_data=(batch, batch, mask, loss_mask),\n",
    "                diffusion_parameters=diffusion_hyperparams,\n",
    "                generate_only_missing=training_config.get(\"only_generate_missing\"),\n",
    "                device=device,\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.cpu().detach().numpy() / len(train_loader)\n",
    "            pbar.set_postfix_str(f\"Loss: {epoch_loss:.4f}\")  # Update progress bar with loss\n",
    "    losses.append(epoch_loss)  # Append epoch loss to main list\n",
    "\n",
    "print(f\"Finished training for {len(losses)} epochs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bfc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(losses) + 1)\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, losses, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246dd035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def std_normal(size: Tuple[int], device: Union[torch.device, str]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate samples from the standard normal distribution of a specified size.\n",
    "\n",
    "    Args:\n",
    "        size (Tuple[int]): Size of the tensor to be generated.\n",
    "        device (Union[torch.device, str]): Device to run the computations on (e.g., 'cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor containing samples from the standard normal distribution.\n",
    "    \"\"\"\n",
    "    return torch.normal(0, 1, size=size).to(device)\n",
    "\n",
    "\n",
    "def sampling(\n",
    "    net: torch.nn.Module,\n",
    "    size: Tuple[int, int, int],\n",
    "    diffusion_hyperparams: Dict[str, torch.Tensor],\n",
    "    cond: torch.Tensor,\n",
    "    mask: torch.Tensor,\n",
    "    only_generate_missing: int = 0,\n",
    "    device: Union[torch.device, str] = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform the complete sampling step according to p(x_0|x_T) = \\prod_{t=1}^T p_{\\theta}(x_{t-1}|x_t).\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The wavenet model.\n",
    "        size (Tuple[int, int, int]): Size of tensor to be generated, usually (number of audios to generate, channels=1, length of audio).\n",
    "        diffusion_hyperparams (Dict[str, torch.Tensor]): Dictionary of diffusion hyperparameters returned by calc_diffusion_hyperparams. Note, the tensors need to be cuda tensors.\n",
    "        cond (torch.Tensor): Conditioning tensor.\n",
    "        mask (torch.Tensor): Mask tensor.\n",
    "        only_generate_missing (int, optional): Flag indicating whether to only generate missing values (default is 0).\n",
    "        device (Union[torch.device, str], optional): Device to place tensors (default is 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The generated audio(s) in torch.Tensor, shape=size.\n",
    "    \"\"\"\n",
    "\n",
    "    _dh = diffusion_hyperparams\n",
    "    T, Alpha, Alpha_bar, Sigma = _dh[\"T\"], _dh[\"Alpha\"], _dh[\"Alpha_bar\"], _dh[\"Sigma\"]\n",
    "    assert len(Alpha) == T\n",
    "    assert len(Alpha_bar) == T\n",
    "    assert len(Sigma) == T\n",
    "    assert len(size) == 3\n",
    "\n",
    "    x = std_normal(size, device)\n",
    "    #print(x.shape, cond.shape)\n",
    "    with torch.no_grad():\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            if only_generate_missing == 1:\n",
    "                x = x * (1 - mask).float() + cond * mask.float()\n",
    "            diffusion_steps = (t * torch.ones((size[0], 1))).to(\n",
    "                device\n",
    "            )  # use the corresponding reverse step\n",
    "            epsilon_theta = net(\n",
    "                (x, cond, mask, diffusion_steps)\n",
    "            )  # predict \\epsilon according to \\epsilon_\\theta\n",
    "            # update x_{t-1} to \\mu_\\theta(x_t)\n",
    "            x = (\n",
    "                x - (1 - Alpha[t]) / torch.sqrt(1 - Alpha_bar[t]) * epsilon_theta\n",
    "            ) / torch.sqrt(Alpha[t])\n",
    "            if t > 0:\n",
    "                x = x + Sigma[t] * std_normal(\n",
    "                    size, device\n",
    "                )  # add the variance term to x_{t-1}\n",
    "\n",
    "    return intercept + std * x, epsilon_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(T - 1, -1, -1):\n",
    "    print((1 - Alpha[t]) / torch.sqrt(1 - Alpha_bar[t])/torch.sqrt(Alpha[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7480a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_dh = diffusion_hyperparams\n",
    "T, Alpha, Alpha_bar, Sigma = _dh[\"T\"], _dh[\"Alpha\"], _dh[\"Alpha_bar\"], _dh[\"Sigma\"]\n",
    "batch = next(iter(test_loader))\n",
    "size = batch.shape\n",
    "cond = batch.to(device)\n",
    "x = std_normal(size, device) * 5 + 100\n",
    "only_generate_missing = 1\n",
    "mask = update_mask(batch).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Assuming batch and generated_series are numpy arrays\n",
    "batch_mean = np.mean(batch.numpy(), axis=0).squeeze()\n",
    "generated_series_mean = np.mean(x.cpu().numpy(), axis=0).squeeze()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(series_length), batch_mean, label='Batch Mean')\n",
    "plt.plot(np.arange(series_length), generated_series_mean, label='Generated Series Mean')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d182ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.animation as animation\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# im = ax.imshow(x.cpu().squeeze(), cmap='gray', vmin=-1, vmax=1)\n",
    "# plt.close()  # Prevents the static plot from displaying\n",
    "\n",
    "# def update_frame(t):\n",
    "#     global x\n",
    "#     with torch.no_grad():\n",
    "#         if only_generate_missing == 1:\n",
    "#             x = x * (1 - mask).float() + cond * mask.float()\n",
    "#         diffusion_steps = (t * torch.ones((size[0], 1))).to(device)\n",
    "#         epsilon_theta = net((x, cond, mask, diffusion_steps))\n",
    "#         x = (x - (1 - Alpha[t]) / torch.sqrt(1 - Alpha_bar[t]) * epsilon_theta) / torch.sqrt(Alpha[t])\n",
    "#         if t > 0:\n",
    "#             x = x + Sigma[t] * std_normal(size, device)\n",
    "#         im.set_array(x.cpu().squeeze())\n",
    "#         return [im]\n",
    "\n",
    "# # Create the animation\n",
    "# ani = animation.FuncAnimation(fig, update_frame, frames=tqdm(range(T - 1, -1, -1)), blit=True)\n",
    "\n",
    "# # Display the animation in the notebook\n",
    "# from IPython.display import HTML\n",
    "# HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a90c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "result = []\n",
    "result2 = []\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(test_loader, desc=f\"Epoch {epoch + 1}\") as pbar:\n",
    "        for batch in pbar:\n",
    "            mask = update_mask(batch)\n",
    "            batch = batch.permute(0, 2, 1)\n",
    "\n",
    "            generated_series, generated_series2 = sampling(\n",
    "                    net=net,\n",
    "                    size=batch.shape,\n",
    "                    diffusion_hyperparams=diffusion_hyperparams,\n",
    "                    cond=batch.to(device),\n",
    "                    mask=mask,\n",
    "                    only_generate_missing=0,\n",
    "                    device=device,\n",
    "                ) \n",
    "            \n",
    "        result.append(generated_series.detach().cpu().numpy().squeeze())\n",
    "        result2.append(generated_series2.detach().cpu().numpy().squeeze())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_result = np.stack(result, axis=0)\n",
    "pred = np.mean(stack_result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming batch and generated_series are numpy arrays\n",
    "batch_mean = np.mean(batch.numpy(), axis=0).squeeze()\n",
    "generated_series_mean = np.mean((pred.squeeze()), axis=0).squeeze()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(series_length), batch_mean, label='Batch Mean')\n",
    "plt.plot(np.arange(series_length), generated_series_mean, label='Generated Series Mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fa717",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_result = np.stack(result, axis=0)\n",
    "pred = np.mean(stack_result, axis=0)\n",
    "pred_med  = np.median(stack_result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b230db",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = test_data[:,-24:, :].transpose(0, 2, 1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = np.mean(test_data[:,:168, :], axis=1)\n",
    "test_std = np.std(test_data[:,:168, :], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(target, pred))\n",
    "print(mean_squared_error(target, pred_med))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_percentage_error(target, pred))\n",
    "print(mean_absolute_percentage_error(target, pred_med))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613090f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate mean and standard deviation for each hour\n",
    "mean_target = np.mean(target, axis=0)\n",
    "std_target = np.std(target, axis=0)\n",
    "mean_pred = np.mean(pred, axis=0)\n",
    "std_pred = np.std(pred, axis=0)\n",
    "\n",
    "# Generate hourly labels\n",
    "hours = np.arange(24)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot target mean with standard deviation band\n",
    "plt.plot(hours, mean_target, label='Target', marker='o')\n",
    "plt.fill_between(hours, mean_target - std_target, mean_target + std_target, alpha=0.2)\n",
    "\n",
    "# Plot prediction mean with standard deviation band\n",
    "plt.plot(hours, mean_pred, label='Prediction', marker='x')\n",
    "plt.fill_between(hours, mean_pred - std_pred, mean_pred + std_pred, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Time Series Comparison: Target vs Prediction')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(hours)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sssd",
   "language": "python",
   "name": "sssd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
